{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c72b663",
   "metadata": {},
   "source": [
    "### Setup env + imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b87228bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup env\n",
    "# pip install/uv add -> pandas numpy scipy matplotlib scikit-learn statsmodels \\ \n",
    "# pandas-market-calendars pytz python-dotenv requests finnhub-python alpaca-trade-api py_vollib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0360b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup all imports\n",
    "import os, time, datetime as dt\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import finnhub\n",
    "from alpaca_trade_api.rest import REST, TimeFrame\n",
    "import yfinance as yf\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ef9ffe",
   "metadata": {},
   "source": [
    "# FinnHub\n",
    "Here we call Finnhub API to get earnings calendar data, and we call Alpaca API to get actual candlestick data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce8532ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "def to_unix(d: dt.datetime) -> int:\n",
    "    return int(time.mktime(d.timetuple()))\n",
    "\n",
    "# fetch daily candles in [open, high, low, close, volume] into a df and index by date\n",
    "def get_daily_candles_finnhub(fh: finnhub.Client, symbol: str, start: dt.datetime, end: dt.datetime) -> pd.DataFrame:\n",
    "    res = fh.stock_candles(symbol, 'D', to_unix(start), to_unix(end))\n",
    "    if res.get('s') != 'ok':\n",
    "        return pd.DataFrame(columns=['open','high','low','close','volume'])\n",
    "\n",
    "    # create a dtindex with UTC, convert, then drop the tz\n",
    "    idx = pd.to_datetime(res['t'], unit='s', utc=True) \\\n",
    "            .tz_convert('US/Eastern').tz_localize(None)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'open':  res['o'],\n",
    "        'high':  res['h'],\n",
    "        'low':   res['l'],\n",
    "        'close': res['c'],\n",
    "        'volume':res['v'],\n",
    "    }, index=idx).sort_index()\n",
    "\n",
    "    return df\n",
    "# fetch daily candles in [open, high, low, close, volume] into a df and index by date\n",
    "\n",
    "def get_daily_candles_alpaca(api: REST, symbol: str, start: dt.datetime, end: dt.datetime) -> pd.DataFrame:\n",
    "    # Alpaca daily bars: pass date-only strings; end is exclusive → add +1 day\n",
    "    start_str = start.date().isoformat()\n",
    "    end_str   = (end.date() + timedelta(days=1)).isoformat()\n",
    "\n",
    "    bars = api.get_bars(symbol, TimeFrame.Day, start=start_str, end=end_str, adjustment='raw').df\n",
    "    if bars.empty:\n",
    "        return pd.DataFrame(columns=['open','high','low','close','volume'])\n",
    "\n",
    "    # Multi-symbol safety\n",
    "    if 'symbol' in bars.columns:\n",
    "        bars = bars[bars['symbol'] == symbol]\n",
    "\n",
    "    # Index is tz-aware UTC; convert to US/Eastern and drop tz for readability\n",
    "    bars.index = bars.index.tz_convert('US/Eastern').tz_localize(None)\n",
    "\n",
    "    # Standardize columns (Alpaca already uses these names, but be explicit)\n",
    "    cols = ['open','high','low','close','volume']\n",
    "    return bars[cols].sort_index()\n",
    "\n",
    "def load_earnings_events_w_augment(fh, symbol: str, want_rows: int = 12) -> pd.DataFrame:\n",
    "    # 1) Finnhub EPS history\n",
    "    eps_hist = fh.company_earnings(symbol, limit=60) or []\n",
    "    eps_df = pd.DataFrame(eps_hist)\n",
    "    if not eps_df.empty:\n",
    "        eps_df['date'] = pd.to_datetime(eps_df['period']).dt.tz_localize(None)\n",
    "        eps_df = eps_df.rename(columns={'estimate':'epsEstimate','actual':'epsActual'})\n",
    "        eps_df = eps_df[['date','epsEstimate','epsActual','surprise','surprisePercent']]\n",
    "    else:\n",
    "        eps_df = pd.DataFrame(columns=['date','epsEstimate','epsActual','surprise','surprisePercent'])\n",
    "\n",
    "    # 2) yfinance dates (more generous history)\n",
    "    yfd = yf.Ticker(symbol).get_earnings_dates(limit=40).reset_index()\n",
    "    yfd = yfd.rename(columns={'Earnings Date':'date'})[['date']]\n",
    "    yfd['date'] = pd.to_datetime(yfd['date']).dt.tz_localize(None)\n",
    "\n",
    "    # 3) union dates, then left-join EPS where present\n",
    "    all_dates = yfd.drop_duplicates('date')\n",
    "    merged = all_dates.merge(eps_df, on='date', how='left').sort_values('date')\n",
    "\n",
    "    # 4) (optional) bring in AMC/BMO for recent window\n",
    "    cal = fh.earnings_calendar(_from=(dt.date.today()-timedelta(days=40)).isoformat(),\n",
    "                               to=(dt.date.today()+timedelta(days=10)).isoformat(),\n",
    "                               symbol=symbol) or {}\n",
    "    cal_df = pd.DataFrame(cal.get('earningsCalendar', []))\n",
    "    if not cal_df.empty and 'date' in cal_df:\n",
    "        cal_df['date'] = pd.to_datetime(cal_df['date']).dt.tz_localize(None)\n",
    "        merged = merged.merge(cal_df[['date','time']], on='date', how='left')\n",
    "    else:\n",
    "        merged['time'] = np.nan\n",
    "\n",
    "    return merged.tail(want_rows).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dac56b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start date:  1998-05-31 , end_date:  2025-10-23\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['time'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m ec = fh.earnings_calendar(_from=start_date, to=end_date, symbol=SYMBOL) \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# events = pd.DataFrame(ec.get('earningsCalendar', []))\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m events = \u001b[43mload_earnings_events_w_augment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSYMBOL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwant_rows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_EARNINGS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(events.tail())\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events.empty:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mload_earnings_events_w_augment\u001b[39m\u001b[34m(fh, symbol, want_rows)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cal_df.empty \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m cal_df:\n\u001b[32m     72\u001b[39m     cal_df[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(cal_df[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m]).dt.tz_localize(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     merged = merged.merge(\u001b[43mcal_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m, on=\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m, how=\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     75\u001b[39m     merged[\u001b[33m'\u001b[39m\u001b[33mtime\u001b[39m\u001b[33m'\u001b[39m] = np.nan\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/quanting-geeks/.venv/lib/python3.11/site-packages/pandas/core/frame.py:4119\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4118\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4119\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4121\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/quanting-geeks/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/quanting-geeks/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['time'] not in index\""
     ]
    }
   ],
   "source": [
    "# Config for now with APLD \n",
    "load_dotenv()\n",
    "SYMBOL = \"APLD\" # change to array of symbols next time\n",
    "LOOKBACK_DAYS = 10000\n",
    "MAX_EARNINGS = 12\n",
    "\n",
    "FH_API_KEY = os.getenv(\"FINNHUB_API_KEY\")\n",
    "\n",
    "ALPACA_KEY = os.getenv(\"ALPACA_API_KEY\")\n",
    "ALPACA_SECRET = os.getenv(\"ALPACA_API_SECRET\")\n",
    "BASE_URL = \"https://paper-api.alpaca.markets\"\n",
    "\n",
    "alpaca = REST(ALPACA_KEY, ALPACA_SECRET, BASE_URL) # type: ignore\n",
    "fh = finnhub.Client(api_key=FH_API_KEY)\n",
    "\n",
    "# 1) pull earnings calendar (past + upcoming window)\n",
    "today = dt.date.today()\n",
    "start_date = (today - timedelta(days=LOOKBACK_DAYS)).isoformat()\n",
    "end_date   = (today + timedelta(days=7)).isoformat()  # include near-future in case an event is imminent\n",
    "print(\"start date: \", start_date, \", end_date: \", end_date)\n",
    "\n",
    "ec = fh.earnings_calendar(_from=start_date, to=end_date, symbol=SYMBOL) or {}\n",
    "# events = pd.DataFrame(ec.get('earningsCalendar', []))\n",
    "events = load_earnings_events_w_augment(fh, SYMBOL, want_rows=MAX_EARNINGS)\n",
    "print(events.tail())\n",
    "\n",
    "if events.empty:\n",
    "    raise RuntimeError(\"No earnings from finnhub in the selected range. Try increasing LOOKBACK_DAYS.\")\n",
    "\n",
    "# keep only the latest N events (historical first)\n",
    "events['date'] = pd.to_datetime(events['date'])\n",
    "events = events.sort_values('date').tail(MAX_EARNINGS).reset_index(drop=True)\n",
    "\n",
    "print(events[['date','epsEstimate','epsActual']])\n",
    "\n",
    "# 2) compute earnings-day gap vs prior close\n",
    "gap_rows = []\n",
    "price_windows = {}  # optional: cache per period\n",
    "\n",
    "for _, row in events.iterrows():\n",
    "    d = row['date'].date()\n",
    "\n",
    "    win_start = dt.datetime.combine(d - timedelta(days=5), dt.time(0,0))\n",
    "    win_end   = dt.datetime.combine(d + timedelta(days=2), dt.time(0,0))\n",
    "    candles = get_daily_candles_alpaca(alpaca, symbol=SYMBOL, start=win_start, end=win_end)\n",
    "    if candles.empty:\n",
    "        continue\n",
    "\n",
    "    # base position: first trading day on/after the calendar date\n",
    "    pos = int(candles.index.searchsorted(pd.Timestamp(d)))\n",
    "\n",
    "    # adjust for timing label: 'amc' → next trading session’s open\n",
    "    time_label = str(row.get('time', '')).lower()\n",
    "    if time_label == 'amc':\n",
    "        pos += 1\n",
    "    # for 'bmo' we keep pos as-is (same day’s open)\n",
    "\n",
    "    # bounds & \"need a previous close\" guard\n",
    "    if pos <= 0 or pos >= len(candles):\n",
    "        continue\n",
    "\n",
    "    prev_close = float(candles.iloc[pos - 1]['close'])\n",
    "    curr_open  = float(candles.iloc[pos]['open'])\n",
    "    idx_e = candles.index[int(pos)]  # DatetimeIndex → Timestamp\n",
    "    gap_pct = (curr_open - prev_close) / prev_close * 100.0\n",
    "\n",
    "    gap_rows.append({\n",
    "        'date': idx_e.date(),\n",
    "        'prev_close': prev_close,\n",
    "        'open_on_earn': curr_open,\n",
    "        'gap_pct': gap_pct,\n",
    "        'eps_est': row.get('epsEstimate', np.nan),\n",
    "        'eps_act': row.get('epsActual', np.nan),\n",
    "        'surprise': row.get('surprise', np.nan),\n",
    "        'surprise_pct': row.get('surprisePercent', np.nan),\n",
    "        'time_label': time_label or np.nan,   # helpful for debugging\n",
    "    })\n",
    "\n",
    "gaps = pd.DataFrame(gap_rows)\n",
    "print(\"Earnings gaps:\")\n",
    "print(gaps)\n",
    "\n",
    "# 3) plot: earnings-day gap %\n",
    "plt.figure(figsize=(9,4.5))\n",
    "plt.bar(gaps['date'].astype(str), gaps['gap_pct'])\n",
    "plt.title(f\"{SYMBOL} earnings-day opening gap (%)\")\n",
    "plt.xlabel(\"Earnings date\")\n",
    "plt.ylabel(\"Gap % (open vs prior close)\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4) plot: EPS surprise (if available)\n",
    "if 'surprise' in gaps and gaps['surprise'].notna().any():\n",
    "    plt.figure(figsize=(9,4.5))\n",
    "    plt.bar(gaps['date'].astype(str), gaps['surprise'])\n",
    "    plt.title(f\"{SYMBOL} EPS surprise (actual - estimate)\")\n",
    "    plt.xlabel(\"Earnings date\")\n",
    "    plt.ylabel(\"EPS surprise\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No EPS surprise data available from the calendar payload for these dates.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quanting-geeks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
